{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Flow\n",
    "\n",
    "Load Data -> Docs -> Divide docs into chunks -> Convert chunks into vector (Vector Embedding) -> Store vectors into VectorStore\n",
    "\n",
    "1. Data Loader - Loading the data\n",
    "2. Data Tranformation -> Divide the loaded data into chunks\n",
    "3. Data Embedding -> Convert chunks data into vectors \n",
    "4. Vector Store -> Store the vectors into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "import bs4 ## Beautiful Scrap -> For WebScrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nOutput Parsers | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Model I/OOutput parsersOutput ParsersOutput parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.Besides having a large collection of different types of output parsers, one distinguishing benefit of LangChain OutputParsers is that many of them support streaming.Quick Start\\u200bSee this quick-start guide for an introduction to output parsers and how to work with them.Output Parser Types\\u200bLangChain has lots of different types of output parsers.Table columns:Name: The name of the output parserSupports Streaming: Whether the output parser supports streaming.Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.Input Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.Output Type: The output type of the object returned by the parser.Description: Our commentary on this output parser and when to use it.NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionOpenAITools(Passes tools to model)Message (with tool_choice)JSON objectUses latest OpenAI function calling args tools and tool_choice to structure the return output. If you are using a model that supports function calling, this is generally the most reliable method.OpenAIFunctions✅(Passes functions to model)Message (with function_call)JSON objectUses legacy OpenAI function calling args functions and function_call to structure the return output.JSON✅✅str | MessageJSON objectReturns a JSON object as specified. You specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML✅✅str | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).CSV✅✅str | MessageList[str]Returns a list of comma separated values.OutputFixing✅str | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError✅str | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame✅str | MessagedictUseful for doing operations with pandas DataFrames.Enum✅str | MessageEnumParses response into one of the provided enum values.Datetime✅str | Messagedatetime.datetimeParses response into a datetime string.Structured✅str | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This useful when you are working with smaller LLMs.Help us out by providing feedback on this documentation page:PreviousTracking token usageNextQuickstartCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content='Output Parsers | 🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content='Skip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Model I/OOutput parsersOutput ParsersOutput parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.Besides having a large'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content='responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.Besides having a large collection of different types of output parsers, one distinguishing benefit of LangChain OutputParsers is that many of them support streaming.Quick Start\\u200bSee this quick-start guide for an introduction to output parsers and how to work with them.Output Parser Types\\u200bLangChain has lots of different types of output parsers.Table columns:Name: The name of the output parserSupports Streaming: Whether the output parser supports streaming.Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.Calls LLM: Whether this output parser itself calls an LLM. This is usually'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content='but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.Input Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.Output Type: The output type of the object returned by the parser.Description: Our commentary on this output parser and when to use it.NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionOpenAITools(Passes tools to model)Message (with tool_choice)JSON objectUses latest OpenAI function calling args tools and tool_choice to structure the return output. If you are using a model that supports function calling, this is generally the most reliable method.OpenAIFunctions✅(Passes functions to model)Message (with function_call)JSON objectUses'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content=\"output. If you are using a model that supports function calling, this is generally the most reliable method.OpenAIFunctions✅(Passes functions to model)Message (with function_call)JSON objectUses legacy OpenAI function calling args functions and function_call to structure the return output.JSON✅✅str | MessageJSON objectReturns a JSON object as specified. You specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML✅✅str | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).CSV✅✅str | MessageList[str]Returns a list of comma separated values.OutputFixing✅str | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError✅str | MessageWraps another output parser. If that output\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content='If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError✅str | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame✅str | MessagedictUseful for doing operations with pandas DataFrames.Enum✅str | MessageEnumParses response into one of the provided enum values.Datetime✅str | Messagedatetime.datetimeParses response into a datetime string.Structured✅str | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/', 'title': 'Output Parsers | 🦜️🔗 LangChain', 'description': 'Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.', 'language': 'en'}, page_content='| Messagedatetime.datetimeParses response into a datetime string.Structured✅str | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This useful when you are working with smaller LLMs.Help us out by providing feedback on this documentation page:PreviousTracking token usageNextQuickstartCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Splitter \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200) # why chunk_overlap \n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "splitted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/vy0rk_tj2fg2rhkts3qkxrq00000gp/T/ipykernel_86936/1022864893.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"gemma2:2b\") # Default -> Llama2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='gemma2:2b', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\") # Default -> Llama2\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x11ac57230>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS \n",
    "faiss_db = FAISS.from_documents(documents=splitted_documents, embedding=embeddings)\n",
    "faiss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Model I/OOutput parsersOutput ParsersOutput parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.Besides having a large'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"There are reasonable limits to concurrent request\"\n",
    "result = faiss_db.similarity_search(query)\n",
    "context = result[0].page_content\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM with RAG\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>{context}</context>    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[0;32m----> 2\u001b[0m document_chain\u001b[38;5;241m=\u001b[39mcreate_stuff_documents_chain(\u001b[43mmodel\u001b[49m,prompt)\n\u001b[1;32m      3\u001b[0m document_chain\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(model,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
