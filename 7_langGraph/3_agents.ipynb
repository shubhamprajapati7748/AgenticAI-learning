{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous router, we invoked the model and, if it chose to call a tool, we returned a ToolMessage to the user.\n",
    "\n",
    "But, what if we simply pass that ToolMessage back to the model?\n",
    "\n",
    "We can let it either (1) call another tool or (2) respond directly.\n",
    "\n",
    "This is the intuition behind ReAct, a general agent architecture.\n",
    "\n",
    "act - let the model call specific tools\n",
    "observe - pass the tool output back to the model\n",
    "reason - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'llvenv (Python 3.12.0)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p d:\\_agenticai\\langchain-learning\\llvenv ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "# llm=ChatGroq(model=\"qwen-2.5-32b\")\n",
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "result=llm.invoke(\"What's your name\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will be the tools \n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add a and b.\n",
    "    \n",
    "    Args:\n",
    "    a : First number\n",
    "    b : Second number\n",
    "    \n",
    "    Returns:\n",
    "    int : Sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract b from a.\n",
    "    \n",
    "    Args:\n",
    "    a : First number\n",
    "    b : Second number\n",
    "    \n",
    "    Returns:\n",
    "    int : Difference between a and b\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple b from a.\n",
    "    \n",
    "    Args:\n",
    "    a : First number\n",
    "    b : Second number\n",
    "    \n",
    "    Returns:\n",
    "    int : Product of a and b\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divide b from a.\n",
    "    \n",
    "    Args:\n",
    "    a : First number\n",
    "    b : Second number\n",
    "    \n",
    "    Returns:\n",
    "    int : Divide of a and b\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, subtract, multiply, divide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM with tools\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls = False) ## parallel_tool_call = false -> For sequence execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the state\n",
    "# from langgraph.graph import MessagesState\n",
    "# or \n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessageState(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages] ## reducer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Prompts \n",
    "from langchain_core.messages import HumanMessage, SystemMessage \n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Arithmeric Assistant \n",
    "def assistant(state:MessageState):\n",
    "    return {\"messages\":[llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Graph \n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder=StateGraph(MessageState)\n",
    "\n",
    "# Defining the node\n",
    "builder.add_node(\"assistant\",assistant)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "## Defining the edges \n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition, )\n",
    "## If the latest messages (result) from assistant is a tool call -> tools_condition routes to tools \n",
    "## If the latest messages from assistant is a tool call -> tools_condition routes to tools \n",
    "\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "## Displaying Graph \n",
    "display(Image(react_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add 10 and 14\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Hello, what is 2 multiplied by 2 then plus 2 then add 4?\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add 10 and 14. Multiply the output by 2. Divide the output by 5\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Memory -> MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "memory = MemorySaver()\n",
    "react_graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"thread_id\" : 1}}\n",
    "\n",
    "messages = [HumanMessage(content=\"What is 3 and 4?\")]\n",
    "messages = react_graph.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Then add 10\")]\n",
    "messages = react_graph.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
