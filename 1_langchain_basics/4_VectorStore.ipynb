{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store\n",
    "- Storing the vectors into DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FAISS\n",
    "- Facebook AI Similarity Search (FAISS)\n",
    "- FAISS is a library for efficient similarity searhc and clustering a dense vectors.\n",
    "- It contains algorithm that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\n",
    "- It also contains supporting code for evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data. \\n\\nHowever, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.\\n\\nAt the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks\\n\\nThe Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations. \\n\\nThe decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Data Load\n",
    "text_loader = TextLoader(\"transformer.txt\")\n",
    "text_docs = text_loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data.'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='For example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. Data Splitting\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "chunks_docs = text_splitter.split_documents(text_docs)\n",
    "chunks_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='gemma2:2b', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3. Embeddings\n",
    "embedding = OllamaEmbeddings(model=\"gemma2:2b\") ## Default mode ls llama2\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x121efa3f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4. FAISS DB\n",
    "faiss_db = FAISS.from_documents(chunks_docs, embedding)\n",
    "faiss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='50ac313f-d335-431b-a4d6-ba251605aec7', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='4b4be241-dd81-436d-9093-b0b48b378111', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       " Document(id='d72c9c63-4042-47e0-abf2-9c5165cd9edd', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(id='34c541df-1afd-42ee-8395-78f7c76866b1', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 5. Query\n",
    "query = \"What is the text all about?\"\n",
    "query_result = faiss_db.similarity_search(query)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a Retriver \n",
    "- We can also convert the vectorStore into a Retriever Class.\n",
    "- This allow us to easily use it in other Langchain models, while largely works with retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='50ac313f-d335-431b-a4d6-ba251605aec7', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='4b4be241-dd81-436d-9093-b0b48b378111', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       " Document(id='d72c9c63-4042-47e0-abf2-9c5165cd9edd', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(id='34c541df-1afd-42ee-8395-78f7c76866b1', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = faiss_db.as_retriever()\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search with Score\n",
    "- These are FAISS specific methods - similarity_search_with_score\n",
    "- This allows to return not only the documents but also the distance score of the query to them.\n",
    "- The returned distanc score is L2 distance (a.k.a Manhatten distance). \n",
    "- Lower Score is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='50ac313f-d335-431b-a4d6-ba251605aec7', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       "  np.float32(3918.3716)),\n",
       " (Document(id='4b4be241-dd81-436d-9093-b0b48b378111', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       "  np.float32(5615.05)),\n",
       " (Document(id='d72c9c63-4042-47e0-abf2-9c5165cd9edd', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       "  np.float32(6339.727)),\n",
       " (Document(id='34c541df-1afd-42ee-8395-78f7c76866b1', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.'),\n",
       "  np.float32(6342.285))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.1227810382843018,\n",
       " 0.6096981763839722,\n",
       " -1.1328717470169067,\n",
       " 0.8620540499687195,\n",
       " 0.19997353851795197,\n",
       " 2.0818614959716797,\n",
       " -1.1502734422683716,\n",
       " -0.23491665720939636,\n",
       " 1.5814882516860962,\n",
       " 0.1362071931362152,\n",
       " -1.6779192686080933,\n",
       " -0.3024980127811432,\n",
       " -1.2110776901245117,\n",
       " -0.7754701972007751,\n",
       " -1.066559076309204,\n",
       " -0.6645272374153137,\n",
       " 0.5272212028503418,\n",
       " -1.0698517560958862,\n",
       " -3.9099199771881104,\n",
       " 0.6610438227653503,\n",
       " 1.159730315208435,\n",
       " -0.3196346163749695,\n",
       " -1.0488115549087524,\n",
       " 0.6563618183135986,\n",
       " 0.37776869535446167,\n",
       " 1.4670379161834717,\n",
       " 0.8872778415679932,\n",
       " -0.6913021206855774,\n",
       " -1.5543091297149658,\n",
       " 0.6368270516395569,\n",
       " -1.9224759340286255,\n",
       " -0.8413493037223816,\n",
       " 0.19763147830963135,\n",
       " 1.2060595750808716,\n",
       " 0.7809198498725891,\n",
       " 1.320129156112671,\n",
       " -0.032588522881269455,\n",
       " -0.3710813820362091,\n",
       " 0.6621020436286926,\n",
       " -1.6674728393554688,\n",
       " 0.2564912438392639,\n",
       " 1.7931692600250244,\n",
       " 0.4910845458507538,\n",
       " -0.36302450299263,\n",
       " -0.5952747464179993,\n",
       " -1.8950824737548828,\n",
       " 1.1311784982681274,\n",
       " -1.8029065132141113,\n",
       " 3.682837963104248,\n",
       " -0.38945499062538147,\n",
       " -0.1813095360994339,\n",
       " -0.16849690675735474,\n",
       " -0.8843321800231934,\n",
       " 4.462541103363037,\n",
       " -1.2043967247009277,\n",
       " -1.6195147037506104,\n",
       " 1.2924418449401855,\n",
       " 3.477933883666992,\n",
       " 0.20677030086517334,\n",
       " -2.535305976867676,\n",
       " -0.5149798393249512,\n",
       " -0.28150707483291626,\n",
       " 0.4778265058994293,\n",
       " 0.6143258810043335,\n",
       " 1.7457733154296875,\n",
       " 1.5136860609054565,\n",
       " 0.019593002274632454,\n",
       " -0.15166059136390686,\n",
       " -1.0769214630126953,\n",
       " -1.4757126569747925,\n",
       " -0.8689368367195129,\n",
       " 1.0938576459884644,\n",
       " -2.607856512069702,\n",
       " -0.8863319158554077,\n",
       " 1.0856741666793823,\n",
       " 0.322928786277771,\n",
       " -1.0180550813674927,\n",
       " -0.8796197175979614,\n",
       " -0.604346513748169,\n",
       " -2.0193774700164795,\n",
       " 0.020547566935420036,\n",
       " 0.24801063537597656,\n",
       " -4.0510053634643555,\n",
       " -1.2882577180862427,\n",
       " -0.026775112375617027,\n",
       " -0.28989240527153015,\n",
       " 0.6303267478942871,\n",
       " -1.4613795280456543,\n",
       " 1.0914307832717896,\n",
       " 0.34246936440467834,\n",
       " -0.5866115093231201,\n",
       " -1.1445502042770386,\n",
       " -0.8802082538604736,\n",
       " 0.5696601867675781,\n",
       " -0.5546066164970398,\n",
       " -1.7586790323257446,\n",
       " -0.2882271707057953,\n",
       " 0.9559961557388306,\n",
       " 0.9258882403373718,\n",
       " 0.10766758024692535,\n",
       " 1.951321005821228,\n",
       " 2.3819150924682617,\n",
       " -1.0380654335021973,\n",
       " 0.6525591611862183,\n",
       " -0.7305397391319275,\n",
       " -1.6095138788223267,\n",
       " -1.596412181854248,\n",
       " 0.7188142538070679,\n",
       " -2.039006233215332,\n",
       " -0.6006726622581482,\n",
       " -0.22619576752185822,\n",
       " 1.3204182386398315,\n",
       " -0.43680495023727417,\n",
       " 2.7286581993103027,\n",
       " -0.09690440446138382,\n",
       " -1.8524854183197021,\n",
       " -0.5275164842605591,\n",
       " 1.8525009155273438,\n",
       " -0.4834788143634796,\n",
       " 0.7834830284118652,\n",
       " 0.929142951965332,\n",
       " -0.1444563865661621,\n",
       " -1.7754099369049072,\n",
       " -1.043879747390747,\n",
       " 0.8102477788925171,\n",
       " -0.03989623859524727,\n",
       " -2.0727767944335938,\n",
       " 0.09667539596557617,\n",
       " -0.3516206741333008,\n",
       " -1.1737360954284668,\n",
       " 0.6235939264297485,\n",
       " 2.1156105995178223,\n",
       " -0.3566089868545532,\n",
       " 1.5456024408340454,\n",
       " -1.8101961612701416,\n",
       " -0.44220608472824097,\n",
       " -0.8406264781951904,\n",
       " 0.7218137383460999,\n",
       " -2.1648032665252686,\n",
       " -1.1315627098083496,\n",
       " 8.689430236816406,\n",
       " -0.8993305563926697,\n",
       " -1.5324422121047974,\n",
       " -1.0846655368804932,\n",
       " 1.545778751373291,\n",
       " -1.6248897314071655,\n",
       " -0.19567328691482544,\n",
       " -0.9651607275009155,\n",
       " 1.725547194480896,\n",
       " -0.07939954102039337,\n",
       " 0.6655572056770325,\n",
       " 0.080220066010952,\n",
       " -1.3668674230575562,\n",
       " -0.5038560628890991,\n",
       " 2.882941722869873,\n",
       " 0.4975006878376007,\n",
       " 0.15467023849487305,\n",
       " 1.0866785049438477,\n",
       " 0.6018907427787781,\n",
       " 0.7492071986198425,\n",
       " -0.5879649519920349,\n",
       " -1.776910662651062,\n",
       " -2.2989447116851807,\n",
       " -0.3776669502258301,\n",
       " 0.33582279086112976,\n",
       " 0.6217814087867737,\n",
       " 0.05566415563225746,\n",
       " -0.5775533318519592,\n",
       " -0.9476094841957092,\n",
       " 0.13988693058490753,\n",
       " -0.6632058620452881,\n",
       " 2.465181589126587,\n",
       " 0.7849564552307129,\n",
       " 0.6246947646141052,\n",
       " -1.1715049743652344,\n",
       " -0.279012531042099,\n",
       " -0.788362443447113,\n",
       " -0.5895494818687439,\n",
       " -0.401710569858551,\n",
       " 0.6385567784309387,\n",
       " -2.1149566173553467,\n",
       " -2.1024608612060547,\n",
       " 1.463147521018982,\n",
       " 0.6249833703041077,\n",
       " 8.840142250061035,\n",
       " 1.0254402160644531,\n",
       " -0.8897741436958313,\n",
       " -0.5954784750938416,\n",
       " 0.4865061640739441,\n",
       " -1.8554632663726807,\n",
       " -0.6044210195541382,\n",
       " -0.38955825567245483,\n",
       " -0.21348892152309418,\n",
       " 0.07147141546010971,\n",
       " -0.4378301501274109,\n",
       " 0.5011722445487976,\n",
       " -1.5882409811019897,\n",
       " -0.3924640715122223,\n",
       " 0.3115229904651642,\n",
       " 0.30957546830177307,\n",
       " 1.2287684679031372,\n",
       " 1.3476524353027344,\n",
       " 1.2943971157073975,\n",
       " 0.5642624497413635,\n",
       " -0.4228842258453369,\n",
       " 0.30380919575691223,\n",
       " -4.220196723937988,\n",
       " -1.0127801895141602,\n",
       " 0.44030675292015076,\n",
       " -1.7731611728668213,\n",
       " 0.5023840665817261,\n",
       " -0.32220569252967834,\n",
       " -0.7694786190986633,\n",
       " -0.9146636724472046,\n",
       " -0.5166807174682617,\n",
       " 0.824623703956604,\n",
       " 0.010195545852184296,\n",
       " -3.1975319385528564,\n",
       " 0.5373789668083191,\n",
       " 1.2750097513198853,\n",
       " -0.2756355404853821,\n",
       " -2.223177909851074,\n",
       " 0.2374894917011261,\n",
       " 1.0312483310699463,\n",
       " -1.5422431230545044,\n",
       " -0.499531090259552,\n",
       " 1.3822873830795288,\n",
       " -2.112759590148926,\n",
       " 0.07946890592575073,\n",
       " -0.3516162931919098,\n",
       " -0.060864225029945374,\n",
       " -1.6613789796829224,\n",
       " -0.7370486855506897,\n",
       " -0.30417880415916443,\n",
       " -1.3232852220535278,\n",
       " -2.286841869354248,\n",
       " -0.468901127576828,\n",
       " 1.1519696712493896,\n",
       " 0.5405173897743225,\n",
       " -0.3556079566478729,\n",
       " -1.599789023399353,\n",
       " 0.5972696542739868,\n",
       " -0.46439677476882935,\n",
       " -1.6339999437332153,\n",
       " -0.4385116696357727,\n",
       " -0.71727454662323,\n",
       " 0.9199405908584595,\n",
       " 0.4248604476451874,\n",
       " -0.8410723209381104,\n",
       " -1.7461098432540894,\n",
       " -0.351058691740036,\n",
       " 1.4542382955551147,\n",
       " 1.7901489734649658,\n",
       " 0.8106375932693481,\n",
       " -0.004586510360240936,\n",
       " -0.604640543460846,\n",
       " 0.36898934841156006,\n",
       " 1.0069632530212402,\n",
       " -1.6343754529953003,\n",
       " 2.171091318130493,\n",
       " -1.8505637645721436,\n",
       " 1.8855928182601929,\n",
       " -2.2951626777648926,\n",
       " 3.714948892593384,\n",
       " -1.892622470855713,\n",
       " -0.902722179889679,\n",
       " 1.8282865285873413,\n",
       " -0.5511435866355896,\n",
       " 0.15422096848487854,\n",
       " 0.4667835831642151,\n",
       " -0.2765684425830841,\n",
       " -0.8872355818748474,\n",
       " -0.5790231227874756,\n",
       " 0.05806044861674309,\n",
       " 0.8708897829055786,\n",
       " -1.6709308624267578,\n",
       " -1.9826968908309937,\n",
       " 2.713974714279175,\n",
       " 0.03654402121901512,\n",
       " 0.7045217156410217,\n",
       " 0.35392868518829346,\n",
       " 1.5669642686843872,\n",
       " 0.9536598920822144,\n",
       " -1.869089961051941,\n",
       " 4.125413417816162,\n",
       " 0.650521993637085,\n",
       " -0.48140090703964233,\n",
       " 0.5899078249931335,\n",
       " 0.5936983227729797,\n",
       " 1.4886990785598755,\n",
       " 1.9614462852478027,\n",
       " 0.4383668899536133,\n",
       " -1.8461675643920898,\n",
       " -0.6630517244338989,\n",
       " -0.8228912353515625,\n",
       " -1.4860159158706665,\n",
       " 1.090058445930481,\n",
       " -0.20699545741081238,\n",
       " 0.8377686738967896,\n",
       " 1.697288990020752,\n",
       " -3.452397346496582,\n",
       " 2.5741472244262695,\n",
       " -1.5420876741409302,\n",
       " 0.5403249859809875,\n",
       " -2.5614659786224365,\n",
       " 0.44379740953445435,\n",
       " -2.1990647315979004,\n",
       " -1.5501885414123535,\n",
       " 0.38123971223831177,\n",
       " 1.3046751022338867,\n",
       " -1.5304661989212036,\n",
       " 0.9591952562332153,\n",
       " -0.5863276720046997,\n",
       " 0.9776541590690613,\n",
       " 1.5065126419067383,\n",
       " -0.6722258925437927,\n",
       " -2.4493801593780518,\n",
       " 1.579779863357544,\n",
       " 1.101030707359314,\n",
       " 1.0128352642059326,\n",
       " -2.47701096534729,\n",
       " 1.07427978515625,\n",
       " 0.12446781247854233,\n",
       " -0.9075548052787781,\n",
       " 0.681549608707428,\n",
       " -3.0426254272460938,\n",
       " 1.8289225101470947,\n",
       " -0.5282089114189148,\n",
       " -0.5078661441802979,\n",
       " 1.4172135591506958,\n",
       " -1.0549730062484741,\n",
       " 4.90873384475708,\n",
       " -0.8747621774673462,\n",
       " 1.0031956434249878,\n",
       " 5.05947732925415,\n",
       " -0.2646857798099518,\n",
       " -2.574455499649048,\n",
       " -0.5048675537109375,\n",
       " -0.5556521415710449,\n",
       " -1.4579421281814575,\n",
       " 2.012549638748169,\n",
       " -2.0337471961975098,\n",
       " 0.10574141144752502,\n",
       " 0.455608606338501,\n",
       " -0.4145537316799164,\n",
       " 0.3205951750278473,\n",
       " 0.6108819842338562,\n",
       " -0.12324324250221252,\n",
       " -0.07415460795164108,\n",
       " 0.9048295021057129,\n",
       " 0.9211340546607971,\n",
       " -0.17694808542728424,\n",
       " 1.4753854274749756,\n",
       " 0.1735825538635254,\n",
       " 1.5969022512435913,\n",
       " 1.4770593643188477,\n",
       " 0.22709529101848602,\n",
       " -0.8852148652076721,\n",
       " 0.45563942193984985,\n",
       " -0.7441690564155579,\n",
       " 1.291978359222412,\n",
       " 0.6677648425102234,\n",
       " 0.20349538326263428,\n",
       " -0.5593031048774719,\n",
       " 6.972957134246826,\n",
       " -0.07719848304986954,\n",
       " -2.806536912918091,\n",
       " -3.5539538860321045,\n",
       " -0.812276303768158,\n",
       " -2.0838263034820557,\n",
       " -1.0325956344604492,\n",
       " -1.757819414138794,\n",
       " -2.1405680179595947,\n",
       " 1.8475100994110107,\n",
       " 1.0634516477584839,\n",
       " 0.1768001914024353,\n",
       " -1.0100865364074707,\n",
       " -0.02058127522468567,\n",
       " 1.0215234756469727,\n",
       " 0.5963290333747864,\n",
       " 1.077048897743225,\n",
       " -1.3052717447280884,\n",
       " 0.7996537685394287,\n",
       " -0.5601271390914917,\n",
       " 0.670080840587616,\n",
       " 2.0690970420837402,\n",
       " 0.6462773084640503,\n",
       " 0.8865906596183777,\n",
       " -0.03586212918162346,\n",
       " 1.190049409866333,\n",
       " 0.49104946851730347,\n",
       " -0.6181684136390686,\n",
       " 0.07266250252723694,\n",
       " 1.8933906555175781,\n",
       " -0.21781131625175476,\n",
       " -0.32624557614326477,\n",
       " 2.1960859298706055,\n",
       " 1.347216010093689,\n",
       " 1.3850818872451782,\n",
       " 0.07341268658638,\n",
       " -0.9784462451934814,\n",
       " 0.7492515444755554,\n",
       " -0.4727020263671875,\n",
       " 0.6698001027107239,\n",
       " -0.3948667645454407,\n",
       " 1.6655735969543457,\n",
       " 0.4302031397819519,\n",
       " -0.44176343083381653,\n",
       " 0.381395161151886,\n",
       " -2.6107711791992188,\n",
       " 0.3210172951221466,\n",
       " -0.3279331922531128,\n",
       " 0.6164886355400085,\n",
       " 1.1998481750488281,\n",
       " -1.3909964561462402,\n",
       " -0.26746004819869995,\n",
       " -0.46948328614234924,\n",
       " -1.0509233474731445,\n",
       " 0.6913256049156189,\n",
       " 1.9386405944824219,\n",
       " 1.2823905944824219,\n",
       " 2.0511574745178223,\n",
       " 0.6560240387916565,\n",
       " -0.22285838425159454,\n",
       " -2.007357358932495,\n",
       " -0.44550567865371704,\n",
       " -0.06248744949698448,\n",
       " 0.4543614685535431,\n",
       " -0.886366069316864,\n",
       " -0.46471264958381653,\n",
       " -4.408270359039307,\n",
       " 1.0421785116195679,\n",
       " 0.44231224060058594,\n",
       " -1.0218257904052734,\n",
       " 0.4064343273639679,\n",
       " -0.14793457090854645,\n",
       " -1.6089926958084106,\n",
       " -0.1918642818927765,\n",
       " 1.05881667137146,\n",
       " 1.581106185913086,\n",
       " -2.351198196411133,\n",
       " 0.010762926191091537,\n",
       " 1.358097791671753,\n",
       " 1.371992826461792,\n",
       " 1.7482112646102905,\n",
       " -0.6845206618309021,\n",
       " 2.0306315422058105,\n",
       " 2.3530778884887695,\n",
       " -0.983600378036499,\n",
       " -1.6781158447265625,\n",
       " -0.16107992827892303,\n",
       " -0.9215556383132935,\n",
       " -0.2319311648607254,\n",
       " -2.228900671005249,\n",
       " 0.397951602935791,\n",
       " -0.0007942215306684375,\n",
       " -0.5101472735404968,\n",
       " 0.1632656455039978,\n",
       " -0.46613824367523193,\n",
       " 1.6676547527313232,\n",
       " -2.6719350814819336,\n",
       " 1.0639580488204956,\n",
       " -0.3942374289035797,\n",
       " -1.0678675174713135,\n",
       " -2.003167152404785,\n",
       " -1.8631854057312012,\n",
       " -0.5284700393676758,\n",
       " 0.3482097387313843,\n",
       " -1.809993028640747,\n",
       " -0.9835094213485718,\n",
       " -0.9922739267349243,\n",
       " 0.11085474491119385,\n",
       " 0.10622264444828033,\n",
       " -0.040847573429346085,\n",
       " -2.1576240062713623,\n",
       " 2.3283002376556396,\n",
       " 2.584568500518799,\n",
       " -2.60794734954834,\n",
       " 0.204116091132164,\n",
       " -1.3944629430770874,\n",
       " 0.06068194657564163,\n",
       " 0.4494236409664154,\n",
       " -0.46631166338920593,\n",
       " -1.930984616279602,\n",
       " 0.7473233342170715,\n",
       " -1.8000532388687134,\n",
       " -1.6375946998596191,\n",
       " 1.8357439041137695,\n",
       " -1.3795279264450073,\n",
       " 0.5924198031425476,\n",
       " -1.4955017566680908,\n",
       " -0.7605975270271301,\n",
       " -0.6302809715270996,\n",
       " -0.5132121443748474,\n",
       " 0.020939987152814865,\n",
       " 2.7179009914398193,\n",
       " -1.2439308166503906,\n",
       " -1.0992389917373657,\n",
       " -1.6135921478271484,\n",
       " -1.4742614030838013,\n",
       " 0.4516424536705017,\n",
       " 0.45974811911582947,\n",
       " -0.9209409356117249,\n",
       " -0.701927900314331,\n",
       " -0.24989980459213257,\n",
       " -0.9569200277328491,\n",
       " -2.938769817352295,\n",
       " 0.6168593764305115,\n",
       " 0.2747347950935364,\n",
       " -0.7450011372566223,\n",
       " 0.1953638792037964,\n",
       " -1.8775173425674438,\n",
       " 1.8338409662246704,\n",
       " -0.23367144167423248,\n",
       " 0.7764935493469238,\n",
       " 2.189197063446045,\n",
       " -0.9647361040115356,\n",
       " -0.05389925092458725,\n",
       " -1.0813307762145996,\n",
       " 1.2434942722320557,\n",
       " -0.9977262020111084,\n",
       " -1.3379178047180176,\n",
       " 1.59444260597229,\n",
       " 1.7917293310165405,\n",
       " -0.11049845069646835,\n",
       " 1.661644697189331,\n",
       " 1.1584662199020386,\n",
       " 0.10772707313299179,\n",
       " -0.9979546666145325,\n",
       " -0.690388023853302,\n",
       " -0.7238672375679016,\n",
       " -2.010596752166748,\n",
       " 0.08920663595199585,\n",
       " 0.6268504858016968,\n",
       " -1.0640615224838257,\n",
       " -1.0930911302566528,\n",
       " -1.5491340160369873,\n",
       " -2.560302734375,\n",
       " -0.44348180294036865,\n",
       " 0.27425462007522583,\n",
       " 6.070339202880859,\n",
       " 0.2555350959300995,\n",
       " 0.09196871519088745,\n",
       " 1.0885776281356812,\n",
       " -0.13875165581703186,\n",
       " 0.8541185259819031,\n",
       " -0.2053600251674652,\n",
       " -0.7550923824310303,\n",
       " 0.39968058466911316,\n",
       " 1.6080087423324585,\n",
       " -1.074953556060791,\n",
       " -1.2510089874267578,\n",
       " -0.01912924088537693,\n",
       " -0.3943682312965393,\n",
       " -0.7164987921714783,\n",
       " -0.9350994229316711,\n",
       " -0.5779477953910828,\n",
       " -0.6387341618537903,\n",
       " -5.2022600173950195,\n",
       " 1.24828040599823,\n",
       " -0.3049888610839844,\n",
       " -0.051297176629304886,\n",
       " 0.14891605079174042,\n",
       " 2.597822904586792,\n",
       " 0.3424164354801178,\n",
       " 0.5859795212745667,\n",
       " -0.41263318061828613,\n",
       " -2.4506728649139404,\n",
       " -0.7932503819465637,\n",
       " -0.11267850548028946,\n",
       " 1.718082308769226,\n",
       " -0.5883618593215942,\n",
       " -1.5882599353790283,\n",
       " -0.2628379464149475,\n",
       " -3.3265998363494873,\n",
       " 1.4925336837768555,\n",
       " -1.4585378170013428,\n",
       " 1.696234107017517,\n",
       " 0.3450952172279358,\n",
       " 0.2828545868396759,\n",
       " 1.751170039176941,\n",
       " 2.208220958709717,\n",
       " 1.9977858066558838,\n",
       " 0.5875692367553711,\n",
       " -1.4128479957580566,\n",
       " 0.24464534223079681,\n",
       " 0.09438203275203705,\n",
       " -0.1955442875623703,\n",
       " -0.7830910086631775,\n",
       " -0.02483135275542736,\n",
       " 2.8341312408447266,\n",
       " 0.6596269607543945,\n",
       " -2.3324241638183594,\n",
       " 37.89671325683594,\n",
       " 0.5396502017974854,\n",
       " -1.6417341232299805,\n",
       " -1.303746223449707,\n",
       " -1.5316108465194702,\n",
       " -0.6021261215209961,\n",
       " -1.745712161064148,\n",
       " 0.012946519069373608,\n",
       " -2.9602625370025635,\n",
       " -0.342008501291275,\n",
       " 1.8975005149841309,\n",
       " -0.3753986954689026,\n",
       " 0.24501286447048187,\n",
       " -1.1924346685409546,\n",
       " -1.856180191040039,\n",
       " -0.02153443545103073,\n",
       " -0.35071825981140137,\n",
       " -0.2902299165725708,\n",
       " 0.33326196670532227,\n",
       " -1.5315462350845337,\n",
       " -0.11812906712293625,\n",
       " -0.43018537759780884,\n",
       " -1.0795799493789673,\n",
       " -0.13379067182540894,\n",
       " 0.814681887626648,\n",
       " -2.588554620742798,\n",
       " -1.0342317819595337,\n",
       " 0.30862343311309814,\n",
       " -0.28190234303474426,\n",
       " 1.4057809114456177,\n",
       " 1.9100334644317627,\n",
       " -1.186577320098877,\n",
       " 0.6787542700767517,\n",
       " -1.0899877548217773,\n",
       " -0.6931558847427368,\n",
       " -1.749807596206665,\n",
       " 0.7474015355110168,\n",
       " 1.808048129081726,\n",
       " -1.3390733003616333,\n",
       " -2.60903263092041,\n",
       " 0.1212197095155716,\n",
       " 0.5786752700805664,\n",
       " 0.27046507596969604,\n",
       " -0.5962443351745605,\n",
       " 0.5932311415672302,\n",
       " 1.3158149719238281,\n",
       " 0.28295454382896423,\n",
       " -0.0507221557199955,\n",
       " -0.28462499380111694,\n",
       " 1.9237515926361084,\n",
       " 1.4069596529006958,\n",
       " -1.044482707977295,\n",
       " -1.7398977279663086,\n",
       " 2.2666468620300293,\n",
       " -0.2705417275428772,\n",
       " -1.1538163423538208,\n",
       " 1.3268113136291504,\n",
       " -1.637222409248352,\n",
       " -2.518146276473999,\n",
       " 1.4372344017028809,\n",
       " -2.4451193809509277,\n",
       " -0.46100515127182007,\n",
       " -0.23281708359718323,\n",
       " -2.501487970352173,\n",
       " -0.8412771224975586,\n",
       " -2.7568559646606445,\n",
       " 0.4141441285610199,\n",
       " -0.04954495653510094,\n",
       " 0.3518441617488861,\n",
       " 0.09768886119127274,\n",
       " -0.2467847466468811,\n",
       " 0.9247996807098389,\n",
       " 0.45770007371902466,\n",
       " 0.3631940186023712,\n",
       " 0.7524948120117188,\n",
       " -0.8473860621452332,\n",
       " 0.11694058030843735,\n",
       " -1.2267917394638062,\n",
       " 0.22431227564811707,\n",
       " -1.4924676418304443,\n",
       " -1.1535428762435913,\n",
       " -0.7535524368286133,\n",
       " -0.25743991136550903,\n",
       " -0.8031212687492371,\n",
       " 1.6704500913619995,\n",
       " 1.8930076360702515,\n",
       " 0.5154188871383667,\n",
       " 0.008965856395661831,\n",
       " -0.0032210256904363632,\n",
       " 1.175510048866272,\n",
       " 1.2456387281417847,\n",
       " -1.0883041620254517,\n",
       " 0.44419941306114197,\n",
       " 0.630938708782196,\n",
       " -1.183272361755371,\n",
       " -0.6470630168914795,\n",
       " -27.77097511291504,\n",
       " 0.060194551944732666,\n",
       " -0.07270295172929764,\n",
       " 0.015154479071497917,\n",
       " -0.40090417861938477,\n",
       " 0.7668052315711975,\n",
       " 0.13857467472553253,\n",
       " 0.7672245502471924,\n",
       " -1.3937695026397705,\n",
       " 1.6022768020629883,\n",
       " -0.1833120584487915,\n",
       " -1.3046756982803345,\n",
       " -0.8437868356704712,\n",
       " 0.9150642156600952,\n",
       " -3.0008187294006348,\n",
       " 1.4748992919921875,\n",
       " -1.4499950408935547,\n",
       " -1.891907811164856,\n",
       " 0.954805314540863,\n",
       " 0.4707896411418915,\n",
       " 4.474063396453857,\n",
       " -0.14626990258693695,\n",
       " 2.5503602027893066,\n",
       " -1.9252244234085083,\n",
       " -1.6717084646224976,\n",
       " 0.35047024488449097,\n",
       " -1.158105731010437,\n",
       " 1.110228419303894,\n",
       " -0.5233162045478821,\n",
       " -0.46182477474212646,\n",
       " -1.0121322870254517,\n",
       " 0.011466506868600845,\n",
       " -0.7146105170249939,\n",
       " -0.1451970338821411,\n",
       " 1.3825135231018066,\n",
       " -1.3952628374099731,\n",
       " -2.730648994445801,\n",
       " 0.41348230838775635,\n",
       " -0.12170539051294327,\n",
       " 0.05793033540248871,\n",
       " 2.134323835372925,\n",
       " 0.15818727016448975,\n",
       " 1.1033732891082764,\n",
       " 1.211820125579834,\n",
       " -1.28195059299469,\n",
       " 0.3163490295410156,\n",
       " -7.13240385055542,\n",
       " -0.47753486037254333,\n",
       " -0.8030000925064087,\n",
       " 0.6016358137130737,\n",
       " -0.3247629404067993,\n",
       " -0.37807610630989075,\n",
       " 0.6352488994598389,\n",
       " 1.2855556011199951,\n",
       " 1.2167125940322876,\n",
       " 1.179674744606018,\n",
       " -1.000103235244751,\n",
       " -1.0228869915008545,\n",
       " -2.9128708839416504,\n",
       " -1.1670464277267456,\n",
       " 0.8736950159072876,\n",
       " -0.5519621968269348,\n",
       " 3.1929285526275635,\n",
       " 0.21015381813049316,\n",
       " 0.5227433443069458,\n",
       " 0.11033227294683456,\n",
       " -1.7063919305801392,\n",
       " 0.2839844524860382,\n",
       " 1.0186876058578491,\n",
       " 1.8222403526306152,\n",
       " -0.5036841630935669,\n",
       " 2.047541618347168,\n",
       " 1.8427411317825317,\n",
       " 1.435161828994751,\n",
       " 0.935577392578125,\n",
       " -0.14173275232315063,\n",
       " -0.6241769790649414,\n",
       " 0.4707987308502197,\n",
       " 1.1911118030548096,\n",
       " 2.4655864238739014,\n",
       " 0.8416152596473694,\n",
       " 0.47365960478782654,\n",
       " -0.5711961984634399,\n",
       " -0.8738439679145813,\n",
       " 0.03244727477431297,\n",
       " -0.17346851527690887,\n",
       " 1.2526944875717163,\n",
       " 0.4263942837715149,\n",
       " -0.7916317582130432,\n",
       " 16.18619155883789,\n",
       " 0.2667132616043091,\n",
       " 0.7705044150352478,\n",
       " 1.101412057876587,\n",
       " 1.0717954635620117,\n",
       " -1.1241004467010498,\n",
       " 0.06471151113510132,\n",
       " -0.8913668394088745,\n",
       " 0.31222671270370483,\n",
       " -0.06516245752573013,\n",
       " 0.2862812578678131,\n",
       " 0.2650740444660187,\n",
       " 2.8293187618255615,\n",
       " 0.8735969662666321,\n",
       " 0.22616153955459595,\n",
       " 0.9753508567810059,\n",
       " -1.2231358289718628,\n",
       " 0.8870115280151367,\n",
       " -0.0572032555937767,\n",
       " -1.0545754432678223,\n",
       " 1.569303274154663,\n",
       " 0.1395522952079773,\n",
       " 0.7235055565834045,\n",
       " -0.8348366022109985,\n",
       " -1.2968777418136597,\n",
       " -1.8994184732437134,\n",
       " -2.1295647621154785,\n",
       " -0.9185872673988342,\n",
       " 0.8629379868507385,\n",
       " -0.7920110821723938,\n",
       " 1.0829083919525146,\n",
       " 0.34617236256599426,\n",
       " -3.0346198081970215,\n",
       " 2.6933963298797607,\n",
       " -0.9188617467880249,\n",
       " 0.3936612904071808,\n",
       " -0.22087135910987854,\n",
       " 3.8202602863311768,\n",
       " 63.88568115234375,\n",
       " 0.689988374710083,\n",
       " -1.3386733531951904,\n",
       " 2.492474317550659,\n",
       " 2.14520263671875,\n",
       " 1.4063795804977417,\n",
       " -0.9585824608802795,\n",
       " -0.6587715148925781,\n",
       " 0.26807233691215515,\n",
       " 1.5723962783813477,\n",
       " 0.5562888383865356,\n",
       " 0.89703369140625,\n",
       " 2.0291402339935303,\n",
       " 1.6271051168441772,\n",
       " -1.217563509941101,\n",
       " -1.34303879737854,\n",
       " -2.4766008853912354,\n",
       " 38.7908821105957,\n",
       " 1.5570863485336304,\n",
       " -2.517329216003418,\n",
       " 0.7854454517364502,\n",
       " -0.44696369767189026,\n",
       " 1.9258816242218018,\n",
       " -1.1951473951339722,\n",
       " -0.032034505158662796,\n",
       " 0.8804275393486023,\n",
       " 3.7564663887023926,\n",
       " -1.1991974115371704,\n",
       " 1.0851222276687622,\n",
       " -0.9279685020446777,\n",
       " -1.108796238899231,\n",
       " 0.1886710226535797,\n",
       " -2.69779896736145,\n",
       " 0.46516114473342896,\n",
       " -0.8737118244171143,\n",
       " -0.3607776463031769,\n",
       " -5.228078365325928,\n",
       " -0.16710759699344635,\n",
       " -1.4900761842727661,\n",
       " 2.420095443725586,\n",
       " 0.18102310597896576,\n",
       " -1.7547703981399536,\n",
       " -0.769537627696991,\n",
       " -1.9088226556777954,\n",
       " 0.18172979354858398,\n",
       " -0.9730790257453918,\n",
       " 1.5367116928100586,\n",
       " -0.18101374804973602,\n",
       " -1.658115029335022,\n",
       " 0.5847908854484558,\n",
       " 0.17075473070144653,\n",
       " 0.27340495586395264,\n",
       " -0.40225666761398315,\n",
       " 0.42190274596214294,\n",
       " -1.7271209955215454,\n",
       " -0.6324884295463562,\n",
       " -1.6444001197814941,\n",
       " -0.42349228262901306,\n",
       " 0.24339978396892548,\n",
       " 1.7112394571304321,\n",
       " -0.3102864921092987,\n",
       " -0.2858881652355194,\n",
       " 0.47250062227249146,\n",
       " -1.0396418571472168,\n",
       " 0.8525894284248352,\n",
       " 2.3399481773376465,\n",
       " -0.012817307375371456,\n",
       " -3.073357343673706,\n",
       " 2.369206666946411,\n",
       " -1.0910875797271729,\n",
       " 0.07111124694347382,\n",
       " 1.45345139503479,\n",
       " -0.7064430713653564,\n",
       " -0.4346085786819458,\n",
       " -0.0007273166556842625,\n",
       " 0.77007657289505,\n",
       " 0.46606409549713135,\n",
       " -1.4589873552322388,\n",
       " -0.031438395380973816,\n",
       " 0.17827646434307098,\n",
       " 0.48667481541633606,\n",
       " 0.6914592385292053,\n",
       " -1.178592562675476,\n",
       " -2.027083396911621,\n",
       " -1.6467441320419312,\n",
       " 1.0524731874465942,\n",
       " 0.4762978255748749,\n",
       " 0.6879065632820129,\n",
       " -0.5674093961715698,\n",
       " -1.634704351425171,\n",
       " 0.45735564827919006,\n",
       " 0.7812399864196777,\n",
       " -0.19957560300827026,\n",
       " 0.13779543340206146,\n",
       " 0.8562361001968384,\n",
       " 0.15049442648887634,\n",
       " 2.2893779277801514,\n",
       " 2.513490676879883,\n",
       " 1.3344004154205322,\n",
       " 0.23672594130039215,\n",
       " 0.359102725982666,\n",
       " -0.3466936945915222,\n",
       " -8.911791801452637,\n",
       " -0.2536584138870239,\n",
       " 1.9635692834854126,\n",
       " -1.5969133377075195,\n",
       " -1.3716830015182495,\n",
       " -0.2594487965106964,\n",
       " -0.5903438925743103,\n",
       " -2.4478650093078613,\n",
       " 0.417289137840271,\n",
       " 0.6458669900894165,\n",
       " -0.31395620107650757,\n",
       " 0.1528526246547699,\n",
       " -4.308746814727783,\n",
       " 2.320876359939575,\n",
       " -0.034562163054943085,\n",
       " -0.3123839497566223,\n",
       " 0.766782820224762,\n",
       " 1.981467843055725,\n",
       " -3.5721774101257324,\n",
       " 0.4853460490703583,\n",
       " 1.6503828763961792,\n",
       " -0.3789466619491577,\n",
       " 0.858613133430481,\n",
       " -0.9021103978157043,\n",
       " -0.030518263578414917,\n",
       " -0.4295448958873749,\n",
       " 1.317674160003662,\n",
       " 1.0389946699142456,\n",
       " -0.89654141664505,\n",
       " -0.1100478395819664,\n",
       " 0.6157541275024414,\n",
       " -0.09392699599266052,\n",
       " 1.3724849224090576,\n",
       " 0.3710904121398926,\n",
       " -3.4196932315826416,\n",
       " 0.5701953172683716,\n",
       " 1.0266677141189575,\n",
       " -0.3138815760612488,\n",
       " -0.6526583433151245,\n",
       " 0.41024336218833923,\n",
       " -1.2026251554489136,\n",
       " 1.0995028018951416,\n",
       " 0.4938550293445587,\n",
       " -1.4928380250930786,\n",
       " 0.14359773695468903,\n",
       " -0.5126340985298157,\n",
       " -2.553551197052002,\n",
       " -0.8244581818580627,\n",
       " -0.09214567393064499,\n",
       " -0.5669361352920532,\n",
       " -3.4743964672088623,\n",
       " -0.21851393580436707,\n",
       " -1.5333452224731445,\n",
       " 0.06924037635326385,\n",
       " -0.6969929337501526,\n",
       " 1.4946011304855347,\n",
       " -1.7647333145141602,\n",
       " -1.6114649772644043,\n",
       " -12.578818321228027,\n",
       " 0.4620966613292694,\n",
       " -0.6760896444320679,\n",
       " -0.4052715301513672,\n",
       " 0.9197607040405273,\n",
       " -0.175857275724411,\n",
       " -2.3253333568573,\n",
       " -0.8011044859886169,\n",
       " 0.018265068531036377,\n",
       " -1.6492671966552734,\n",
       " 0.21516191959381104,\n",
       " 0.43467262387275696,\n",
       " -1.4277278184890747,\n",
       " -0.7041050791740417,\n",
       " -0.3165134787559509,\n",
       " 2.1776628494262695,\n",
       " -0.7800923585891724,\n",
       " 0.7013612985610962,\n",
       " 1.0930657386779785,\n",
       " -1.467887282371521,\n",
       " 0.3193832039833069,\n",
       " 0.05144238471984863,\n",
       " 1.566328525543213,\n",
       " -1.72457754611969,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector = embedding.embed_query(query)\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='50ac313f-d335-431b-a4d6-ba251605aec7', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='4b4be241-dd81-436d-9093-b0b48b378111', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       " Document(id='d72c9c63-4042-47e0-abf2-9c5165cd9edd', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(id='34c541df-1afd-42ee-8395-78f7c76866b1', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_db.similarity_search_by_vector(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading FAISS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving\n",
    "faiss_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x122f10380>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loading\n",
    "new_db = FAISS.load_local(\"faiss_index\", embedding, allow_dangerous_deserialization=True)\n",
    "new_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChromaDB\n",
    "- Chroma is a AI-native open-source vector database focused on developer productivity and happiness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x12613bce0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db = Chroma.from_documents(chunks_docs, embedding)\n",
    "chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7e00e49c-3f20-4279-bfc7-e17639d1dc8f', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='d85f6615-08ce-49d6-a6d4-81e9f4ba0eba', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       " Document(id='826b06dc-811d-4f0f-9dd0-6063853eca5e', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(id='718956bb-a9ed-4c36-897c-4dcaceb0c124', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## query it \n",
    "query = \"What is the text all about?\"\n",
    "query_result = chroma_db.similarity_search(query)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving\n",
    "vectordb = Chroma.from_documents(documents=chunks_docs, embedding=embedding, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x12638d280>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loading from disk\n",
    "choma_db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\n",
    "choma_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c5e1930b-1c07-40a3-b0b1-f676040d3834', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='3a176a52-2e86-4113-bcf0-79d0302f078a', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       " Document(id='330b74ef-7a6c-4e97-8967-1b752116a112', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(id='90a92cb5-c794-4c1e-88a2-de4aa46afe75', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = choma_db2.similarity_search(query)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='c5e1930b-1c07-40a3-b0b1-f676040d3834', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       "  3918.3715654670036),\n",
       " (Document(id='3a176a52-2e86-4113-bcf0-79d0302f078a', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       "  5615.049209089743),\n",
       " (Document(id='330b74ef-7a6c-4e97-8967-1b752116a112', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       "  6339.727036409713),\n",
       " (Document(id='90a92cb5-c794-4c1e-88a2-de4aa46afe75', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.'),\n",
       "  6342.284773675256)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = choma_db2.similarity_search_with_score(query)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c5e1930b-1c07-40a3-b0b1-f676040d3834', metadata={'source': 'transformer.txt'}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='3a176a52-2e86-4113-bcf0-79d0302f078a', metadata={'source': 'transformer.txt'}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'),\n",
       " Document(id='330b74ef-7a6c-4e97-8967-1b752116a112', metadata={'source': 'transformer.txt'}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(id='90a92cb5-c794-4c1e-88a2-de4aa46afe75', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrievar\n",
    "retriever = choma_db2.as_retriever()\n",
    "retriever.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
