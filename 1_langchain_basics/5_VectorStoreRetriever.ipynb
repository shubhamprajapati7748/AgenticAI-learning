{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data. \\n\\nHowever, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.\\n\\nAt the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks\\n\\nThe Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations. \\n\\nThe decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "loader = TextLoader('transformer.txt')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data. \\n\\nHowever, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks'),\n",
       " Document(metadata={'source': 'transformer.txt'}, page_content='This ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks\\n\\nThe Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations. \\n\\nThe decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_documents(docs)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/vy0rk_tj2fg2rhkts3qkxrq00000gp/T/ipykernel_4612/1022864893.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"gemma2:2b\") # Default -> Llama2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='gemma2:2b', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\") # Default -> Llama2\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "vector_store\n",
    "\n",
    "## retriever \n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"There are reasonable limits to concurrent request\"\n",
    "result = vector_store.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['LANGSMITH_API_KEY'] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ['LANGSMITH_PROJECT'] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ['LANGSMITH_TRACING'] = os.getenv(\"LANGSMITH_TRACING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x129ee5310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x129ee6420>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"Gemma2-9b-It\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"##  AI Agents: Autonomous Software for Your Digital World \\n\\nAI agents are essentially **software programs that can act autonomously to achieve specific goals**. They are designed to perceive their environment, make decisions, and take actions based on learned patterns and given instructions. \\n\\nThink of them as **virtual assistants** with a more sophisticated brain. \\n\\nHere's a breakdown:\\n\\n**Key Features:**\\n\\n* **Autonomy:** They operate independently, without constant human intervention.\\n* **Goal-Oriented:** They are programmed with specific objectives and work towards achieving them.\\n* **Environment Interaction:** They can sense and react to changes in their surroundings (data, user input, etc.).\\n* **Decision Making:** They use algorithms and learned knowledge to choose the best course of action.\\n* **Learning and Adaptation:** Many AI agents can learn from their experiences and improve their performance over time.\\n\\n**Examples:**\\n\\n* **Chatbots:**  Provide customer service, answer questions, and engage in conversations.\\n* **Recommendation Systems:** Suggest products, movies, or articles based on user preferences.\\n* **Self-Driving Cars:** Navigate roads, avoid obstacles, and transport passengers.\\n* **Game AI:** Control non-player characters (NPCs) in video games, making them more realistic and challenging.\\n* **Fraud Detection Systems:** Analyze financial transactions to identify suspicious activity.\\n\\n**Types of AI Agents:**\\n\\n* **Reactive Agents:** Respond directly to immediate stimuli without considering past experiences.\\n* **Proactive Agents:** Anticipate future events and take actions to achieve their goals.\\n* **Learning Agents:** Adapt their behavior based on feedback and experience.\\n* **Social Agents:** Interact and collaborate with other agents.\\n\\n**Benefits:**\\n\\n* **Automation:** Free up human time and resources for more complex tasks.\\n* **Efficiency:**  Optimize processes and improve productivity.\\n* **Personalization:**  Tailor experiences and recommendations to individual users.\\n* **Innovation:**  Enable new possibilities in fields like healthcare, education, and entertainment.\\n\\n**Challenges:**\\n\\n* **Bias:** AI agents can inherit biases from the data they are trained on, leading to unfair or discriminatory outcomes.\\n* **Transparency:**  Understanding how AI agents make decisions can be difficult, raising concerns about accountability and trust.\\n* **Security:**  AI agents can be vulnerable to attacks, potentially causing harm if compromised.\\n\\n**The Future:**\\n\\nAI agents are rapidly evolving and becoming more sophisticated. As research progresses, we can expect to see even more innovative applications in the years to come.\\n\\n\\nLet me know if you have any more questions about AI agents!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 543, 'prompt_tokens': 14, 'total_tokens': 557, 'completion_time': 0.987272727, 'prompt_time': 8.9019e-05, 'queue_time': 0.020716817000000002, 'total_time': 0.987361746}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a49aa91-4de9-451e-86f2-3ea97c442d8e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 543, 'total_tokens': 557})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## input and get response from LLM \n",
    "result = llm.invoke(\"What is AI Agents?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='What is transformer?:\\n\\n{context}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x129ee5310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x129ee6420>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"What is transformer?:\\n\\n{context}\")]\n",
    ")\n",
    "\n",
    "## create_stuff_documents_chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='What is transformer?:\\n\\n{context}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x129ee5310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x129ee6420>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8431e022-bf50-4540-8daa-fcaf949b3a91', metadata={'source': 'transformer.txt'}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks'),\n",
       " Document(id='a379f66d-1d36-4f4d-9148-7086ac12ebc4', metadata={'source': 'transformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data. \\n\\nHowever, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(id='c8262723-17b4-4451-bf7a-5ebade9381a5', metadata={'source': 'transformer.txt'}, page_content='This ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks\\n\\nThe Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations. \\n\\nThe decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"What is transfomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x128907590>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='What is transformer?:\\n\\n{context}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x129ee5310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x129ee6420>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s a great explanation of transformers! Here\\'s a concise summary:\\n\\n**What is a Transformer?**\\n\\nA transformer is a type of neural network architecture specifically designed for processing sequential data, like text. It revolutionized natural language processing (NLP) due to its unique ability to understand relationships between words in a sentence, regardless of their distance.\\n\\n**Key Features:**\\n\\n* **Self-Attention:** This is the heart of the transformer. It allows each word in a sequence to \"attend\" to all other words, weighing their importance in understanding the context. Imagine it like each word having a conversation with all the other words, figuring out who\\'s most relevant.\\n* **Encoder-Decoder Structure:**  Transformers typically have two main parts:\\n    * **Encoder:**  Processes the input sequence, understanding its meaning and relationships.\\n    * **Decoder:**  Generates the output sequence (e.g., translating a sentence, writing a summary) based on the encoder\\'s understanding.\\n* **Parallel Processing:** Unlike older models like RNNs, transformers can process entire sequences simultaneously, making them much faster to train.\\n\\n**Why are Transformers Important?**\\n\\nTransformers have achieved state-of-the-art results in a wide range of NLP tasks, including:\\n\\n* Machine Translation\\n* Text Summarization\\n* Question Answering\\n* Chatbots\\n\\nThey have become the dominant architecture in NLP due to their ability to capture long-range dependencies and complex relationships within text.\\n\\n**Examples of Transformer Models:**\\n\\n* BERT\\n* GPT-3\\n* T5\\n* LaMDA\\n\\n\\n\\nLet me know if you have any other questions!\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=retrieval_chain.invoke({\"input\":\"Why transformer\"})\n",
    "result['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
